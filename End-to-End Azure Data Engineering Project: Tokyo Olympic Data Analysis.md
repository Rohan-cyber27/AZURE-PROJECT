## End-to-End Azure Data Engineering Project: Tokyo Olympic Data Analysis
This guide outlines the steps to analyze Tokyo Olympic data using Azure services, including Azure Storage, Azure Databricks, Azure Data Factory, Azure Synapse Analytics, and Power BI. The data consists of CSV files containing athlete details, gender, medals, coaches, and teams. We will transform, store, and analyze this data.

# Prerequisites
# Azure Account: Ensure you have an active Azure subscription.
# Required Services: Azure Storage, Azure Databricks, Azure Data Factory, App Registration, Azure Synapse Analytics, and Power BI.
Step-by-Step Instructions

# 1. Set Up Azure Storage Account
Create a Storage Account:
Navigate to the Azure Portal.
Go to Storage accounts and click + Add.
Configure the storage account:
Subscription: Select your subscription.
Resource Group: Choose or create a new resource group.
Storage account name: Enter a unique name.
Region: Select the region.
Click Review + Create and then Create.
Create Containers and Directories:

# Navigate to the created storage account.
Go to Containers and click + Container.
Create a container named rawdata for storing raw data.
Inside the rawdata container, create a directory named data.

# 2. Configure Azure Data Factory
Create an Azure Data Factory Instance:
In the Azure Portal, go to Azure Data Factory and click + Create.
Fill in the details:
Name: Choose a name for your Data Factory.
Subscription: Select your subscription.
Resource Group: Select the existing resource group.
Region: Choose the region.
Click Review + Create and then Create.
Create a Pipeline to Transfer Data:

# Navigate to the Data Factory instance.
Go to Author & Monitor to launch the Data Factory Studio.
In the Author & Monitor section, click + New pipeline.
Drag the Copy Data activity to the pipeline canvas.
Configure the source and sink:
Source: Set the source to HTTP and paste the GitHub raw URL for your CSV files.
Sink: Set the sink to the rawdata container and data directory.

# 3. Configure App Registration for Databricks
Create App Registration:
Go to Azure Active Directory > App registrations > + New registration.
Enter the name, and select Accounts in this organizational directory only.
Click Register.
Create Client Secret:

# Navigate to Certificates & secrets.
Click + New client secret, enter a description, and set an expiry date.
Click Add and copy the Value (secret key) for later use.
Copy Details for Databricks Authentication:
Copy the Application (client) ID, Directory (tenant) ID, and Client Secret.

# 4. Set Up Azure Databricks
Create a Databricks Workspace:
Go to Azure Databricks and click Create Workspace.
Fill in the details:
Workspace Name: Enter a name.
Resource Group: Select your resource group.
Region: Choose the region.
Click Create.
Launch Databricks Studio:

Go to the created Databricks workspace and click Launch Workspace.
In Databricks Studio, create a Cluster:
Go to Clusters > Create Cluster.
Enter a name, select the cluster configuration, and click Create Cluster.
Create a Notebook and Write Transformation Code:

Navigate to Workspace > Users > Your Name > Create > Notebook.
Name the notebook, select Python as the language, and click Create.
Write Python code to read, transform, and write data. Example code:
python
Copy code
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("OlympicsData").getOrCreate()

# Read data from storage
raw_data_path = "dbfs:/mnt/rawdata/data/"
df = spark.read.csv(raw_data_path + "olympics_data.csv", header=True, inferSchema=True)

Transform data
transformed_df = df.select("Athlete", "Gender", "Medal", "Team", "Coach")

Write transformed data to storage
transformed_data_path = "dbfs:/mnt/transformeddata/"
transformed_df.write.format("parquet").save(transformed_data_path)

spark.stop()
Mount Storage to Databricks:

# 5. Set Up Azure Synapse Analytics
Create a Synapse Workspace:
Go to Azure Synapse Analytics and click + Create.
Fill in the details:
Workspace Name: Enter a name.
Resource Group: Select your resource group.
Region: Choose the region.
Click Review + Create and then Create.
Add Linked Service to Synapse:

Navigate to Manage > Linked services > + New.
Select Azure Blob Storage, configure the connection with your storage account details, and click Create.
Create a SQL Pool:

Go to Manage > SQL pools > + New.
Configure the SQL pool with the desired settings and click Create.
Run SQL Queries on Data:

# Navigate to Develop > SQL scripts.
Write and execute SQL queries on your transformed data. Example query:
sql
Copy code
SELECT * FROM transformeddata.olympics_data;

# 6. Connect to Power BI
Create a Power BI Workspace:
Go to Power BI Service and sign in.
Click Create > Workspace and configure your workspace.
Get Data from Synapse Analytics:

In Power BI Desktop, click Get Data > Azure > Azure Synapse Analytics.
Enter the server name, database name, and authentication details. Click Connect.
Build Your Dashboard:

Import tables/views from Synapse Analytics.
Use Power BIâ€™s visualizations to create reports and dashboards.
Example visualizations might include charts for medals by country, athlete statistics, and medal trends over time.
Publish and Share Dashboard:

Click File > Publish > To Power BI.
Select the workspace and publish your report. Share the dashboard link with stakeholders.
Conclusion

successfully set up an end-to-end data engineering pipeline for analyzing Tokyo Olympic data. The process involved configuring Azure Storage, Data Factory, Databricks, Synapse Analytics, and Power BI. You can now explore further enhancements, such as advanced data transformations, more complex queries, and interactive Power BI reports.

